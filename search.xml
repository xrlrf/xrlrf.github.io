<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python网络爬虫与信息提取(三)：网络爬虫之实战]]></title>
    <url>%2F2017%2F10%2F09%2FPython%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96-%E4%B8%89-%EF%BC%9A%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B9%8B%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[Re(正则表达式)库入门 regular expression = regex = RE 是一种通用的字符串表达框架,用来简洁表达一组字符串的表达式,也可用来判断某字符串的特征归属 正则表达式的语法 常用操作符 实例 Re库的基本使用 正则表达式的表示类型为raw string类型(原生字符串类型),表示为r’text’ Re库主要功能函数 功能函数 re.search(pattern,string,flags=0) re.match(pattern,string,flags=0) 因为match为从开始位置开始匹配,使用时要加if进行判别返回结果是否为空,否则会报错 re.findall(pattern,string,flags=0) re.split(pattern,string,maxsplit=0,flags=0) maxsplit为最大分割数,剩余部分作为最后一个元素输出 re.finditer(pattern,string,flags=0) re.sub(pattern,repl,string,count=0,flags=0) repl是用来替换的字符串,count为替换次数 Re库的另一种等价用法 Re库的函数式用法为一次性操作,还有一种为面向对象用法,可在编译后多次操作 1regex = re.compile(pattern,flags=0) 通过compile生成的regex对象才能被叫做正则表达式 Match对象的属性 Match对象的方法 例子 Re库的贪婪匹配和最小匹配 Re库默认采取贪婪匹配,即输出匹配最长的子串 实例二:淘宝商品比价定向爬虫(requests-re) 步骤1:提交商品搜索请求,循环获取页面 步骤2:对于每个页面,提取商品名称和价格信息 步骤3:将信息输出显示 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import requestsimport redef getHTMLText(url): try: r = requests.get(url, timeout=30) r.raise_for_status() r.encoding = r.apparent_encoding return r.text except: return &quot;&quot;def parsePage(ilt, html): try: plt = re.findall(r&apos;\&quot;view_price\&quot;\:\&quot;[\d\.]*\&quot;&apos;,html) tlt = re.findall(r&apos;\&quot;raw_title\&quot;\:\&quot;.*?\&quot;&apos;,html) for i in range(len(plt)): price = eval(plt[i].split(&apos;:&apos;)[1]) title = eval(tlt[i].split(&apos;:&apos;)[1]) ilt.append([price , title]) except: print(&quot;&quot;)def printGoodsList(ilt): tplt = &quot;&#123;:4&#125;\t&#123;:8&#125;\t&#123;:16&#125;&quot; print(tplt.format(&quot;序号&quot;, &quot;价格&quot;, &quot;商品名称&quot;)) count = 0 for g in ilt: count = count + 1 print(tplt.format(count, g[0], g[1]))def main(): goods = &apos;书包&apos; depth = 3 start_url = &apos;https://s.taobao.com/search?q=&apos; + goods infoList = [] for i in range(depth): try: url = start_url + &apos;&amp;s=&apos; + str(44*i) html = getHTMLText(url) parsePage(infoList, html) except: continue printGoodsList(infoList)main() 实例三:股票数据定向爬虫 步骤1:从东方财富网获取股票列表 步骤2:根据股票列表逐个到百度股票获取个股信息 步骤3:将结果存储到文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#CrawBaiduStocksB.pyimport requestsfrom bs4 import BeautifulSoupimport tracebackimport redef getHTMLText(url, code=&quot;utf-8&quot;): try: r = requests.get(url) r.raise_for_status() r.encoding = code return r.text except: return &quot;&quot;def getStockList(lst, stockURL): html = getHTMLText(stockURL, &quot;GB2312&quot;) soup = BeautifulSoup(html, &apos;html.parser&apos;) a = soup.find_all(&apos;a&apos;) for i in a: try: href = i.attrs[&apos;href&apos;] lst.append(re.findall(r&quot;[s][hz]\d&#123;6&#125;&quot;, href)[0]) except: continuedef getStockInfo(lst, stockURL, fpath): count = 0 for stock in lst: url = stockURL + stock + &quot;.html&quot; html = getHTMLText(url) try: if html==&quot;&quot;: continue infoDict = &#123;&#125; soup = BeautifulSoup(html, &apos;html.parser&apos;) stockInfo = soup.find(&apos;div&apos;,attrs=&#123;&apos;class&apos;:&apos;stock-bets&apos;&#125;) name = stockInfo.find_all(attrs=&#123;&apos;class&apos;:&apos;bets-name&apos;&#125;)[0] infoDict.update(&#123;&apos;股票名称&apos;: name.text.split()[0]&#125;) keyList = stockInfo.find_all(&apos;dt&apos;) valueList = stockInfo.find_all(&apos;dd&apos;) for i in range(len(keyList)): key = keyList[i].text val = valueList[i].text infoDict[key] = val with open(fpath, &apos;a&apos;, encoding=&apos;utf-8&apos;) as f: f.write( str(infoDict) + &apos;\n&apos; ) count = count + 1 print(&quot;\r当前进度: &#123;:.2f&#125;%&quot;.format(count*100/len(lst)),end=&quot;&quot;) except: count = count + 1 print(&quot;\r当前进度: &#123;:.2f&#125;%&quot;.format(count*100/len(lst)),end=&quot;&quot;) continuedef main(): stock_list_url = &apos;http://quote.eastmoney.com/stocklist.html&apos; stock_info_url = &apos;https://gupiao.baidu.com/stock/&apos; output_file = &apos;D:/BaiduStockInfo.txt&apos; slist=[] getStockList(slist, stock_list_url) getStockInfo(slist, stock_info_url, output_file)main()]]></content>
      <categories>
        <category>Python学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python网络爬虫与信息提取(二)：网络爬虫之提取]]></title>
    <url>%2F2017%2F10%2F09%2FPython%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96-%E4%BA%8C-%EF%BC%9A%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B9%8B%E6%8F%90%E5%8F%96%2F</url>
    <content type="text"><![CDATA[Beautiful Soup库可对HTML/XML格式进行解析并提取相关信息 安装: pip install beautifulsoup4 小测： 1234567&gt;&gt;&gt; import requests&gt;&gt;&gt; r = requests.get(&quot;http://python123.io/ws/demo.html&quot;)&gt;&gt;&gt; r.text&gt;&gt;&gt; demo = r.text&gt;&gt;&gt; from bs4 import BeautifulSoup&gt;&gt;&gt; soup = BeautifulSoup(demo,&quot;html.parser&quot;)&gt;&gt;&gt; print(soup.prettify()) Beautiful Soup库是解析/遍历/维护”标签熟”的功能库,引用方式: 12from bs4 import BeautifulSoupimport bs4 Beautiful Soup库的4种解析器: Beautiful Soup类的基本元素: bs类基本元素 Tag标签任何存在于HTML语法中的标签都可用**soup.访问获得,存在多个取第一个 Tag的name每个有自己的名字,通过.name获取,字符串类型 Tag的attrs Tag的NavigableString Tag的Comment 由find_all()扩展的七个方法: 实例一:中国大学排名爬虫 123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/env python# coding=utf-8import requestsfrom bs4 import BeautifulSoupimport bs4def getHTMLText(url): try: r = requests.get(url,timeout=30) r.raise_for_status() r.encoding=r.apparent_encoding return r.text except: return &quot;error&quot;def fillUnivList(ulist,html): soup=BeautifulSoup(html,&quot;html.parser&quot;) for tr in soup.find(&apos;tbody&apos;).children: if isinstance(tr,bs4.element.Tag): tds = tr(&apos;td&apos;) ulist.append([tds[0].string,tds[1].string,tds[2].string])def printUnivList(ulist,num): tplt=&quot;&#123;0:^10&#125;\t&#123;1:&#123;3&#125;^10&#125;\t&#123;2:^10&#125;&quot; print(tplt.format(&quot;排名&quot;,&quot;学校名称&quot;,&quot;总分&quot;,chr(12288))) for i in range(num): u=ulist[i] print(tplt.format(u[0],u[1],u[2],chr(12288)))def main(): uinfo=[] url = &quot;http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html&quot; html = getHTMLText(url) fillUnivList(uinfo,html) printUnivList(uinfo,20)main()]]></content>
      <categories>
        <category>Python学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python网络爬虫与信息提取(一)]]></title>
    <url>%2F2017%2F10%2F09%2FPython%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[Requests库的七个主要方法: get方法 r = requests.get(url):右边构造一个向服务器请求资源的Requests对象,左边返回一个包含服务器资源的Response对象给r 完整参数:requests.get(url,params=None,**kwargs),实则由request方法封装 Response对象的五个属性: 爬取网页的通用代码框架 Requests库爬取网页会遇到异常: 6种常见异常： 使用r.raise_for_status()方法构建通用代码框架: 12345678910#!/usr/bin/env python# coding=utf-8def getHTMLText(url)try: r = request.get(url,timeout = 30) r.raise_for_status() r.encoding = r.apparent_encoding return r.textexcept: return &quot;产生异常&quot; HTTP协议对资源的操作: Requests库主要方法:requests.request(method,url,kwargs)** method(请求方式)包括: GET/HEAD/POST/PUT/PATCH/delete/OPTIONS **kwargs(控制访问参数)包括: params(添加键值到url后)/data(字典/字节序列等作为Request的内容)/json/headers(HTTP定制头)/cookies(Request中的cookie)/auth(元祖,支持HTTP认证)/files(传输文件)/timeout/proxies(设定访问代理服务器)/allow_redirects(重定向开关)/stream(获取内容立即下载开关)/verify(认证SSL证书开关)/cert(本地SSL证书路径) Requests库爬取实例 京东商品页面的爬取 123456789import requestsurl = &quot;http://item.jd.com/2967929.html&quot;try: r=requests.get(url) r.raise_for_status() r.encoding=r.apparent_encoding print(r.text[:1000])except: print(&quot;爬取失败&quot;) 亚马逊商品页面的爬取由于亚马逊有自身的头部审查,故我们模拟浏览器访问: 12345678910import requestsurl = &quot;http://www.amazon.cn/gp/product/B01M8L5Z3Y&quot;try: kv = &#123;&apos;user-agent&apos;:&apos;Mozilla/5.0&apos;&#125; r= requests.get(url,headers = kv) r.raise_for_status() r.encoding=r.apparent_encoding print(r.text[1000:2000])except: print(&quot;爬取失败&quot;) 百度/360搜索关键词提交首先我们需要知道搜索关键词的提交接口:百度:http://www.baidu.com/s?wd=keyword360:http://www.so.com/s?q=keyword接下来我们可以利用params参数将关键词加入,代码如下: 12345678910import requestskeyword = &quot;Python&quot;try: kv = &#123;&apos;wd&apos;:keyword&#125; r= requests.get(&quot;http://www.baidu.com/s&quot;,params = kv) print(r.request.url) r.raise_for_status() print(len(r.text))except: print(&quot;爬取失败&quot;) 网络图片的爬取和存储 123456789101112131415161718192021#!/usr/bin/env python# coding=utf-8import requestsimport osurl = &quot;http://image.nationalgeographic.com.cn/2017/0311/20170311024522382.jpg&quot;root = &quot;/home/xiaorui/文档/Python/&quot;path = root +url.split(&apos;/&apos;)[-1]try: if not os.path.exists(root): os.mkdir(root) if not os.path.exists(path): r=requests.get(url) with open(path,&apos;wb&apos;) as f: f.write(r.content) f.close() print(&quot;文件保存成功&quot;) else: print(&quot;文件已存在&quot;)except: print(&quot;爬取失败&quot;) IP地址归属地的查询 1234567891011#!/usr/bin/env python# coding=utf-8import requestsurl = &quot;http://m.ip138.com/ip.asp?ip=&quot;try: r=requests.get(url+&apos;202.204.80.112&apos;) r.raise_for_status() r.encoding=r.apparent_encoding print(r.text[-500:])except: print(&quot;爬取失败&quot;)]]></content>
      <categories>
        <category>Python学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown新手指南手册]]></title>
    <url>%2F2017%2F10%2F09%2Fmarkdown%E6%96%B0%E6%89%8B%E6%8C%87%E5%8D%97%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[标题123456# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题 列表无序列表123- 文本1- 文本2- 文本3 有序列表1231. 文本12. 文本23. 文本3 链接1[显示文本](链接地址) 图片1![](图片链接地址) 引用1&gt; 需要引用的文本 粗体1**文本** 斜体1*文本* 代码引用用两个、、、包围需要引用的代码即可 表格12345|Tables |Are |Cool ||-----------------|:-----------|----||col 3 is |right-aligned | $1600 ||col 2 is |centered | $12 ||zebra stripes | are neat | $1 | 显示效果： Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1]]></content>
      <categories>
        <category>Linux学习</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Next主题 文章添加阅读次数，访问量等]]></title>
    <url>%2F2017%2F10%2F06%2FHexo-Next%E4%B8%BB%E9%A2%98-%E6%96%87%E7%AB%A0%E6%B7%BB%E5%8A%A0%E9%98%85%E8%AF%BB%E6%AC%A1%E6%95%B0%EF%BC%8C%E8%AE%BF%E9%97%AE%E9%87%8F%E7%AD%89%2F</url>
    <content type="text"><![CDATA[本章所讲给文章设置阅读量，启用不蒜子统计，仅限于文章页面显示阅读书，在首页不显示。效果如下图所示： 打开 Hexo 目录下的 \themes\next\ _config.yml 文件]]></content>
      <categories>
        <category>Hexo学习</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown 编辑器： remarkable 安装(ubuntu)]]></title>
    <url>%2F2017%2F10%2F06%2Fmarkdown-%E7%BC%96%E8%BE%91%E5%99%A8%EF%BC%9A-remarkable-%E5%AE%89%E8%A3%85-ubuntu%2F</url>
    <content type="text"><![CDATA[下载安装包 http://remarkableapp.github.io/linux/download.html 安装之： 1dpkg -i remarkable_1.62_all.deb 补上依赖项： 1sudo apt-get install -f 运行： 1remarkable &amp;]]></content>
      <categories>
        <category>安装软件</category>
      </categories>
      <tags>
        <tag>软件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客搭建（next主题系列详解）]]></title>
    <url>%2F2017%2F10%2F06%2FHexo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%EF%BC%88next%E4%B8%BB%E9%A2%98%E7%B3%BB%E5%88%97%E8%AF%A6%E8%A7%A3%EF%BC%89%2F</url>
    <content type="text"><![CDATA[hexo的next主题配置详解]]></content>
      <categories>
        <category>Hexo学习</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F10%2F05%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment /** * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/ /* var disqus_config = function () { this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable }; */ (function() { // DON'T EDIT BELOW THIS LINE var d = document, s = d.createElement('script'); s.src = 'https://https-xrlrf-github-io.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); Please enable JavaScript to view the comments powered by Disqus. window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-107582762-1');]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客使用系列（四）——文章里嵌入音乐播放器]]></title>
    <url>%2F2017%2F10%2F05%2FHexo%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8%E7%B3%BB%E5%88%97%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94%E6%96%87%E7%AB%A0%E9%87%8C%E5%B5%8C%E5%85%A5%E9%9F%B3%E4%B9%90%E6%92%AD%E6%94%BE%E5%99%A8%2F</url>
    <content type="text"><![CDATA[内嵌播放器 打开网页版网易云选择自己喜欢的音乐或歌单，点开歌曲名或者歌单名，点击生成外链播放器，赋值html代码，将html代码无需任何修改放入markdown文章就可以了。可以取消自动播放。]]></content>
      <categories>
        <category>hexo学习</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python/Django生成二维码]]></title>
    <url>%2F2017%2F10%2F05%2FPython%E7%94%9F%E6%88%90%E4%BA%8C%E7%BB%B4%E7%A0%81%2F</url>
    <content type="text"><![CDATA[开始安装1.1 用Python来生成二维码很简单，可以看 qrcode 这个包：1pip install qrcode qrcode 依赖 Image 这个包： 1pip install Image 1.2 安装后就可以使用了，这个程序带了一个 qr 命令：1qr &apos;http://www.ziqiangxuetang.com&apos; &gt; test.png 1.3 下面我们看一下如何在 代码 中使用1234567import qrcodeimg = qrcode.make(&apos;http://www.tuweizhong.com&apos;)# img &lt;qrcode.image.pil.PilImage object at 0x1044ed9d0&gt;with open(&apos;test.png&apos;, &apos;wb&apos;) as f: img.save(f) 安装： 12pip install git+git://github.com/ojii/pymaging.git#egg=pymagingpip install git+git://github.com/ojii/pymaging-png.git#egg=pymaging-png 使用方法大致相同，命令行上： 1qr --factory=pymaging &quot;Some text&quot; &gt; test.png Python中调用： 123import qrcodefrom qrcode.image.pure import PymagingImageimg = qrcode.make(&apos;Some data here&apos;, image_factory=PymagingImage) /** * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/ /* var disqus_config = function () { this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable }; */ (function() { // DON'T EDIT BELOW THIS LINE var d = document, s = d.createElement('script'); s.src = 'https://https-xrlrf-github-io.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); Please enable JavaScript to view the comments powered by Disqus. window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-107582762-1');]]></content>
      <categories>
        <category>Python学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
</search>
